{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1661851-1dfb-43f1-b481-2dde467530e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from torch.utils.data import TensorDataset, SequentialSampler, DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.nn import functional as F\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a40720d4-a13b-4f4c-86b1-dea0d20b1cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './bert_save_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d83519da-c062-4b81-b246-372ade1fb742",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81ecd1d2-3990-4db6-89fd-5dff3d28e2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec37e20-9e7f-4daf-b8a6-630d96a35d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26432cc-ccc0-45c9-81fd-07cb34cc1b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b3a036-8e91-4cd4-ba03-2e265e3a11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd1e8c39-0d4a-47ca-aaa3-94db14632aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_tokenize(text):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28ee889a-549e-4e11-b738-0a4e58187bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(tokens):\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    processed_text = \" \".join(tokens)\n",
    "    processed_text = \" \".join(processed_text.split())\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "334f546c-c245-40a1-ae2c-f2daca3ab7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Davidson_modified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8386416-aca4-430c-a45a-4a98ae33982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet'] = data['tweet'].apply(tweet_tokenize)\n",
    "data['tweet'] = data['tweet'].apply(preprocess_text)\n",
    "sentences = data.tweet.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c24fcbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfe520a1-8f02-4407-8039-96dcc2ba4fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96492b38-fac7-4d61-92ca-5b0b0d802358",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sentences:\n",
    "\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_length,           # Pad & truncate all sentences.\n",
    "                        padding = 'max_length',\n",
    "                        truncation = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "# Set the batch size.  \n",
    "batch_size = 16  \n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c73ab8-9739-4be6-8811-c5a4c821b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "predictions  = []\n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask = batch\n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions.append(logits)\n",
    "stack = np.vstack(predictions)\n",
    "final_preds = F.softmax(torch.from_numpy(stack), dim=1)[:, 1].numpy()\n",
    "data['predictions'] = final_preds\n",
    "print(data[['tweet', 'predictions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d59e7-2b12-44c9-acc9-8287ed05ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "binary_predictions = (final_preds > 0.5).astype(int)\n",
    "\n",
    "labels_true = data['class'].values\n",
    "\n",
    "accuracy = accuracy_score(labels_true, binary_predictions)\n",
    "f1 = f1_score(labels_true, binary_predictions)\n",
    "recall = recall_score(labels_true, binary_predictions)\n",
    "precision = precision_score(labels_true, binary_predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "data['predictions'] = binary_predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
